Add to abstract section:

The method of using client-side SSD caches on distributed storage systems has been limited mostly to write-through configurations. Even though write-back SSD caching provides better I/O performance by reducing the overhead write requests from a faster storage device instead of the slower storage device, the potential risks of data loss coming from client failure or cache device failure can deter users from adopting these configurations in their systems. This project offers a strategy for increasing reliability guarantees of write-back caches by implementing cross-client cooperative caching and replication policies. The results of this research could potentially allow for a wider adoption of write-back caches in storage systems.

(Section 3.2) The two large paragraphs I highlighted should be replaced by these two: 


 In order to implement the cooperative caching mechanism we plan to instrument dm-cache with a module extension that allows it communicate block request data from the dm-cache module in one client to another’s. The previous idea was to modify a version of the iSCSI initiator and target modules, but after further inspection of the structure of these two iSCSI modules, we found them to be complex. Specifically, their kernel implementation and their dependence on the SCSI storage architecture made it unfeasible for us to learn about this for this project. iSCSI is designed to send SCSI commands over network, when our goal was to send blocks of  data. Thus, we abandoned the idea of modifying the iSCSI modules. Consequently, for our initial prototype we plan to use an open-source implementation of kernel sockets, and use these to write an interface that allows dm-cache to be used both as a client to initiate the propagation of replicas and as a server to monitor the arrival of replicas sent by other clients.


 When loading our version of DM-Cache on a client machine, it will have the chance to specify a circle of peer clients to which it can propagate replicas. A circle is basically a group of clients which have agreed to let another peer client use its free cache space. A client needs only the IP, the port, and a special identifier of all the clients that are part of the circle in order to connect to them. When a propagation request is activated, a client will send out a request to each client in its circle and it will wait for each client to respond with a notification, letting it know whether it has any space available in its cache, and whether the replica will be stored or rejected. The client that propagated the data will then decide based whether the operation was successful or not based on the established degree of replication. 

For Current Progress: 

For the past few weeks our work has focused on implementing the initial prototype for our project. Rather than start implementing a version of the prototype that used replication across a network, our goal was to implement a version of dm-cache that was able to replicate its dirty data among two cache devices that were local to a machine. By doing so we were going to be able to observe the performance overhead that replication would have. We researched the different ways we could do this and found that for this purpose we could modify the portions of dm-cache where a dirty cache write is produced, and add a special function provided by device-mapper. The function was dm_io_sync_bvec, and it was supposed to allow us to send different portions of a bio to different devices in a synchronous manner.  However, this still has yet to work as all of our tests show that the function blocks infinitely and fails to return for simple cache misses. We are in the process of debugging this issue, but if little progress is made by the weekend we will proceed to implement the networking-based cooperative caching mechanism.

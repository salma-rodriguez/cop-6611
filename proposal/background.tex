\section{Background}

Replication techniques for distributed systems have been
explored on many papers. Optimistic replication has been devised
in order to increase availability, while allowing
concurrent access to data\cite{Replication}. Although
previous research has focused on improving availability on
distributed\cite{globally-distributed} and grid computing
systems\cite{Online-Grid}, reliability for distributed caching
systems has had little improvement.

\textit{Pessimistic replication} is a synchronous replication
technique that guarantees a high degree of consistency and
reliability by blocking access to data until all destinations
have received a consistent copy of the data. However, pessimistic
replication increases the variability in access time, since
network behavior and hardware failure are highly unpredictable.
As a result, latency spikes are expected to occur more frequently.
In the case of DM Cache, using pessimistic replication would
result in higher latency when there are more clients cooperating
as replication targets. The scalability problem with pessimistic
replication is shown in \cite{Replicated-Services}, as well as
\cite{Replication-Cost}.

The work of Saito and Shapiro on 
optimistic replication\cite{Replication} shows how to achieve
eventual consistency on distributed systems while making the
data highly available across different locations on a geographic
network. However, there is no warranty for reliability, since the
data is replicated asynchronously, with the optimistic
assumption that the target will receive the data.

Our focus for this paper is not to achieve high
availability, but we will use some of the ideas behind optimistic
replication in order to maintain some availability. In all cases,
it is not possible to make the system both highly available and
highly reliable. To have good reliability there must be at least one
consistent copy of the data after every update. Hence, reliability
is directly proportional to the number of consistent replicas
in a distributed caching system, where each client has access
to its own data and the data of no other client.

We employ a technique that is similar to
that used in \cite{dynamo}, where a fixed number of clients
holding the replica are required to respond before proceeding with
subsequent operations. The number can be dynamically changed
according to frequency of access, so that data that is written to
more frequently mandates a higher number of responses for each
replication operation. Other approaches, including optimistic
replication, can be statistically compared to this approach, in
order to determine which makes the system more reliable, and which
is best under which circumstances.

Consistent hashing has been explored in many works of research.
By using a key-value approach, the hash function can be designed
so that there is an upper bound to the number of keys that each
node is responsible for. An advantage to using a circular hashing
mechanism is that nodes can join or exit the network
dynamically, and affect only the nearest successor\cite{chord}.
If time allows it, we will explore consistent hashing and how
it can improve reliability by giving each client a fair chance
to replicate its content.

\label{background}
